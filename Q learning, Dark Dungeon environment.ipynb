{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create environment where the observation space is a 5 elements list\n",
    "#action space is 2 discrete, 0 leads to cell 0 in the list with reward +2, and 1 leads to one step forward. When we reach\n",
    "#the 5th cell in the list we get +10 reward\n",
    "#landing on any other state leads to reward=0\n",
    "#each episode ends after 60 steps\n",
    "#applying action 0 in state 0 leads to staying in state 0\n",
    "#applying action 1 in state 5 leads to staying in state 1\n",
    "class DarkDungeon(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.observation_space=Discrete(5)\n",
    "        self.action_space=Discrete(2)\n",
    "        self.state = self.observation_space.sample()\n",
    "        self.steps=60\n",
    "        \n",
    "    \n",
    "    def step(self, action):\n",
    "        reward=0\n",
    "        if action==0:\n",
    "            new_state = 0\n",
    "            reward += 2\n",
    "        else:\n",
    "            new_state = self.state+1\n",
    "            \n",
    "        self.steps -= 1\n",
    "        \n",
    "            \n",
    "        if new_state>=4:\n",
    "            reward += 10\n",
    "            new_state=4\n",
    "            \n",
    "        if self.steps==0:\n",
    "            done=True\n",
    "        else:\n",
    "            done=False\n",
    "            \n",
    "        info={}\n",
    "        return new_state, reward, done, info\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state= self.observation_space.sample()\n",
    "        self.steps=60\n",
    "        return self.state\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create q table containing 0 values\n",
    "env = DarkDungeon()\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes=10000\n",
    "epsilon=1\n",
    "gamma = 0.95\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q learning\n",
    "rewards= 0\n",
    "rewardsAcrossEpisodes = []\n",
    "#go through episodes\n",
    "#at each episode we reset the environment, and don't forget to set done to False\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    r = 0\n",
    "    \n",
    "    while(True):\n",
    "        #apply epsilon-greedy strategy\n",
    "        num = random.uniform(0,1)\n",
    "        if num < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        #apply the chosen action to the environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #update the value of the q_table according to: old_q = old_q + learning_rate*(reward + discount_factor*max(new_q) - old_q)\n",
    "        q_table[state, action] += learning_rate*(reward + gamma * np.max(q_table[new_state, :]) - q_table[state, action])\n",
    "        \n",
    "        #update state to become next state\n",
    "        #update reward\n",
    "        state = new_state\n",
    "        r += reward\n",
    "        rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewardsAcrossEpisodes.append(r)\n",
    "            epsilon = epsilon*0.9\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.        ,  39.92172721],\n",
       "       [ 40.        ,  29.16199156],\n",
       "       [ 40.        ,  22.89602341],\n",
       "       [ 40.        ,  19.855799  ],\n",
       "       [ 54.15349765, 200.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning_rate = 0.1\n",
    "#epsilon=0.017\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 200.        ,  199.92364052],\n",
       "       [ 200.        ,  161.14544863],\n",
       "       [ 200.        ,  169.69356421],\n",
       "       [ 200.        ,  197.8396305 ],\n",
       "       [ 581.95146909, 1000.        ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epsilon=1\n",
    "#decay *0.8\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.        ,  39.94262726],\n",
       "       [ 40.        ,  29.70858818],\n",
       "       [ 40.        ,  14.98555348],\n",
       "       [ 41.88458789, 200.        ],\n",
       "       [ 79.34358404, 200.        ]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epsilon=1\n",
    "#decay *0.9\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 40.        ,  39.66091185],\n",
       "       [ 40.        ,  39.89858087],\n",
       "       [ 40.        ,  39.89914819],\n",
       "       [ 40.35017992, 200.        ],\n",
       "       [ 61.00768426, 200.        ]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning_rate = 0.1\n",
    "#epsilon=1 decay 0.99\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 200.        ,  199.76342597],\n",
       "       [ 200.        ,  199.51399788],\n",
       "       [ 200.        ,  199.5170842 ],\n",
       "       [ 200.40249308, 1000.        ],\n",
       "       [ 554.51272084, 1000.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#epsilon=1\n",
    "#decay *0.99\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract optimal policy\n",
    "optimal_policy = []\n",
    "for state in range(env.observation_space.n):\n",
    "    optimal_policy.append(np.argmax(q_table[state, :]))\n",
    "    \n",
    "optimal_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
